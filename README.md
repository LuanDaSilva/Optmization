# Optmization
Some of the algorithms of the book Algorithms for optmization from Michael Kochenderfer transcribed to Python

Algorithms implemented until now:

1- Adaptative Simulated Annealing<br />
2 - Augmented Lagrange<br />
3 - BFGS<br />
4 - Bracket Line search ( Search Algorithm)<br />
5 - Bracket search<br />
6 - Covariance Matrix Adaptation<br />
7 - Conjugate Gradient<br />
8 - Cross Entropy Method<br />
9 - Cyclic Coordinates descent<br />
10 - Differential Evolution<br />
11 - Firefly Search<br />
12 - Gauss Newton<br />
13 - Genetic Algorithms<br />
14 - Gradient/Hessians calculator<br />
15 - Golden_Search<br />
16 - Gradient Descent<br />
17 - Gradient Descent with momentum methods<br />
18 - Hooke Jeeevs<br />
19 - Interior Point Method<br />
20 - Levenberg Marquadt<br />
21 - MADS<br />
22 - Matthew Daves algorithm for positive definite matrices<br />
23 - Natural evolution strategies<br />
24 - Nelder-Mead Simplex<br />
25 - Newton Method<br />
26 - Particle Swarm Optmization<br />
27 - Penalty Method<br />
28 - Powell method(CG)<br />
29 - Stong Backtracking<br />
... and a decorator to check the performance

To check the algorithms, one should use the <a href="https://en.wikipedia.org/wiki/Test_functions_for_optimization"> Test Functions </a> 


